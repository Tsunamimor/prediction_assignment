---
title: "Predicting exercise activities"
author: "H. S."
date: "20 January 2019"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## Summary

In this report, we build a model to predict the way in which a certain exercise is performed, using measurements of the motion as predictors. Methods used include cross validiation and random forests.



## Getting and cleaning the data

We begin by downloading the relevant (training and quizz test) data and loading it into R.

```{r, cache=TRUE}
if(!file.exists("pml-training.csv"))
{
    url.training <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    download.file(url = url.training, destfile="pml-training.csv")
}
train <- read.csv("pml-training.csv")

if(!file.exists("pml-testing.csv"))
{
    url.testing <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    download.file(url = url.testing, destfile="pml-testing.csv")
}
quizzTest <- read.csv("pml-testing.csv")
```

In a first step of cleaning the data, we will remove all the columns which -- in the data needed for the quizz -- exceed an NA percentage of 10%. Moreover, we discard columns 1 to 6, since they only specify properties of the measurement (e.g. time of measurement, test person etc.) and thus do not contain information on the action performed by the test person.

```{r}
N <- dim(train)[1]
na.columns <- sapply(quizzTest, function(x){ sum(is.na(x))/20 }) > 0.1

train <- train[,!na.columns]
quizzTest <- quizzTest[,!na.columns]

train <- train[,-(1:6)]
quizzTest <- quizzTest[,-(1:6)]
```



## Partitioning the data and preparation for cross validation

The cleaned data is now partitioned into a training set (60%) and a test set (40%).

```{r}
library(caret)
set.seed(100)
inTrain <- createDataPartition(y=train$classe, p=0.6, list=FALSE)
training <- train[inTrain,]
test <- train[-inTrain,]
```

In order to apply cross validation in the next step, we partition the training data into k=3 disjoint subsets of approximately equal size. The parameter k is chosen to be 3 for the sake of simplicity, enabling us to showcase the application of cross validation in a fairly simple scenario.

```{r}
crossValPartition <- createFolds(y=training$classe, k=3, list=FALSE)

trainingA <- training[crossValPartition!=1,]
testA <- training[crossValPartition==1,]

trainingB <- training[crossValPartition!=2,]
testB <- training[crossValPartition==2,]

trainingC <- training[crossValPartition!=3,]
testC <- training[crossValPartition==3,]
```



## Learning the models and model selection with cross validation

In each step of the cross validation procedure, we take k-1=2 of the k=3 training sets from above as a joint training set (see preceding paragraph) to learn the model, using the respective last partition as a test set to predict on. As a model, we will use a random forest approach, since it is known to be quite accurate in prediction/classification scenarios as ours.

```{r}
library(randomForest)

modelA <- randomForest(classe~., data=trainingA)
predictA <- predict(modelA, newdata=testA)

modelB <- randomForest(classe~., data=trainingB)
predictB <- predict(modelB, newdata=testB)

modelC <- randomForest(classe~., data=trainingC)
predictC <- predict(modelC, newdata=testC)
```

We are now able to select the best model by calculating the prediction error (measured by the percentage of wrongly classified cases) on the corresponding test sets. (Note that we did not use the original test set, i.e. the other 40% of the data, as that set is reserved for the final step of calculating the out-of-sample error estimate.)

```{r}
mean(predictA != testA$classe)
mean(predictB != testB$classe)
mean(predictC != testC$classe)
```

Apparently modelA has the smallest error when evaluated on the corresponding test set, which is why we will use modelA as a prediction model in the rest of the report.



## Evaluation and prediction

With the best model chosen, we will now estimate the out-of-sample error by predicting on the test set.

```{r}
predictTest <- predict(modelA, newdata=test)
mean(predictTest != test$classe)
```

Finally, we use the chosen model (i.e. modelA) to predict the activities on the data needed for the quizz.

```{r}
predict(modelA, newdata=quizzTest)
```